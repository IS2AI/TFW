{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to image\n",
    "image_path = 'images/4076.png'\n",
    "# path to model\n",
    "model_path = 'yolov5_weights/yolov5m6.pt'\n",
    "# image size\n",
    "img_size = 512\n",
    "# intersection over union threshold\n",
    "iou_thr = 0.5\n",
    "# confidence score threshold\n",
    "con_thr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(weights, device):\n",
    "    model = attempt_load(weights, map_location=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a device and load the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(img, xyxy, conf):\n",
    "    h,w,c = img.shape\n",
    "    tl = 1 or round(0.004 * (h + w) / 2) + 1  # line/font thickness\n",
    "    x1 = int(xyxy[0])\n",
    "    y1 = int(xyxy[1])\n",
    "    x2 = int(xyxy[2])\n",
    "    y2 = int(xyxy[3])\n",
    "    cv2.rectangle(img, (x1,y1), (x2, y2), (0,255,0), thickness=tl, lineType=cv2.LINE_AA)\n",
    "\n",
    "    tf = max(tl - 1, 1)  # font thickness\n",
    "    cv2.putText(img, str(conf)[:4], (x1, y1 - 2), 0, tl / 1.5, [0, 0, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_one(model, device, image_path, img_size, con_thr, iou_thr):\n",
    "    # load the image and convert it \n",
    "    # from BGR to a 3 channel grayscale image\n",
    "    orgimg = cv2.imread(image_path)  \n",
    "    orgimg = cv2.cvtColor(orgimg, cv2.COLOR_BGR2GRAY)\n",
    "    orgimg = np.dstack([orgimg] * 3)\n",
    "    \n",
    "    # make a copy of the original image\n",
    "    img0 = orgimg.copy()\n",
    "    \n",
    "    h0, w0 = orgimg.shape[:2]  # orig hw\n",
    "    r = img_size / max(h0, w0)  # resize image to img_size\n",
    "    if r != 1:  # always resize down, only resize up if training with augmentation\n",
    "        interp = cv2.INTER_AREA if r < 1  else cv2.INTER_LINEAR\n",
    "        img0 = cv2.resize(img0, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "\n",
    "    # check img_size\n",
    "    imgsz = check_img_size(img_size, s=model.stride.max()) \n",
    "    img = letterbox(img0, new_shape=imgsz)[0]\n",
    "\n",
    "    # Preprocess the image\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1).copy()\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    pred = model(img)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, con_thr, iou_thr)\n",
    "        \n",
    "    # Process predictions\n",
    "    for det in pred:  # per image\n",
    "        # Rescale boxes from img_size to im0 size\n",
    "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], orgimg.shape).round()\n",
    "        \n",
    "        # extract bounding boxes to draw them\n",
    "        # on the original image\n",
    "        for j in range(det.size()[0]):\n",
    "            bounding_box = det[j, :4].numpy().tolist()\n",
    "            conf = det[j, 4].cpu().numpy()\n",
    "            orgimg = show_results(orgimg, bounding_box, conf)\n",
    "            \n",
    "    img = Image.fromarray(cv2.cvtColor(orgimg, cv2.COLOR_BGR2RGB))\n",
    "    img.save('yolov5.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_one(model, device, image_path, img_size, con_thr, iou_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
